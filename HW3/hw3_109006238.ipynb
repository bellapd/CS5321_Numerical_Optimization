{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "894384f9",
   "metadata": {},
   "source": [
    "# Numerical Optimization (CS215300) Assignment 3\n",
    "## Introduction\n",
    "\n",
    "Name: Annabella Putri Dirgo\n",
    "\n",
    "ID: 109006238\n",
    "\n",
    "In this assignment, we expect you to be able to solve constrained optimization problem by Scipy library. We want you to apply two algorithms on the following problem as a benchmark, survey the methods used in these libraries, and analyze the behavior of these algorithms.\n",
    "The library document link: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "\n",
    "## Task\n",
    "1. (20%) Solve the following problrem by using **trust-constr** method:\n",
    "$$\\begin{array}{lll}\n",
    "        \\min_{x_1,x_2} & f(x_1,x_2)=-x_1-x_2 \\\\\n",
    "        s.t.  & -2x_1^4 + 8x_1^3 -8x_1^2 + x_2 - 2 \\le 0  \\\\\n",
    "         & -4x_1^4 + 32x_1^3 - 88x_1^2 + 96x_1 + x_2 -36 \\le 0   \\\\\n",
    "         & 0 \\le x_1 \\le 3 \\\\\n",
    "         & 0 \\le x_2 \\le 4 \\\\\n",
    "\\end{array}$$\n",
    "2. (20%) Use **COBYLA** method to solve the same problem.\n",
    "3. (15%) For the above two algorithms, please include the calculation process in markdown style before your code cells.\n",
    "4. (5%) Provide the Jacobian and Hessian function in matrix form in markdown style.\n",
    "5. (40%) In your report, please read the paper cited in the libraries, which gives the details of the algorithms. Write an introduction of the algorithms, and compare their behaviors in this benchmark. You are not necessarily to read the original paper, other resourses (ex. slides from other schools or surveys) are also acceptable. Please include the link or paper name in your reference if you referred to other resources.\n",
    "6. Rename this notebook file by adding your student ID and upload it to eeclass platform. (ex. hw3_110xxxxxx.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c364c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea454f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from autograd import jacobian, hessian\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f49385",
   "metadata": {},
   "source": [
    "### Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908e2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -x[0]-x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248684d",
   "metadata": {},
   "source": [
    "### Define constraint functions and derivatives\n",
    "Note: Please do not use sparse matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "680da280",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\mathrm{f}(\\mathrm{x}, \\mathrm{y})=\\left[\\begin{array}{c}\n",
    "-2 \\mathrm{x}^4+8 \\mathrm{x}^3-8 \\mathrm{x}^2+\\mathrm{y}-2 \\\\\n",
    "-4 \\mathrm{x}^4+32 \\mathrm{x}^3-88 \\mathrm{x}^2+96 \\mathrm{x}+\\mathrm{y}-36\n",
    "\\end{array}\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "979cd323",
   "metadata": {},
   "source": [
    "### Jacobian function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23d5abcf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\mathrm{f}_1(\\mathrm{x}, \\mathrm{y})=-2 \\mathrm{x}^4+8 \\mathrm{x}^3-8 \\mathrm{x}^2+\\mathrm{y}-2 \\\\\n",
    "& \\mathrm{f}_2(\\mathrm{x}, \\mathrm{y})=-4 \\mathrm{x}^4+32 \\mathrm{x}^3-88 \\mathrm{x}^2+96 \\mathrm{x}+\\mathrm{y}-36 \\\\\n",
    "& \\mathrm{~J}_{\\mathrm{f}}(\\mathrm{x}, \\mathrm{y})=\\left[\\begin{array}{ll}\n",
    "\\frac{\\partial \\mathrm{f}_1}{\\partial \\mathrm{x}} & \\frac{\\partial \\mathrm{f}_1}{\\partial \\mathrm{y}} \\\\\n",
    "\\frac{\\partial \\mathrm{f}_2}{\\partial \\mathrm{x}} & \\frac{\\partial \\mathrm{f}_2}{\\partial \\mathrm{y}}\n",
    "\\end{array}\\right] \\\\\n",
    "& \\mathrm{J}_{\\mathrm{f}}(\\mathrm{x}, \\mathrm{y})=\\left[\\begin{array}{cc}\n",
    "-8 \\mathrm{x}^3+24 \\mathrm{x}^2-16 \\mathrm{x} & 1 \\\\\n",
    "-16 \\mathrm{x}^3+96 \\mathrm{x}^2-176 \\mathrm{x}+96 & 1\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39675d25",
   "metadata": {},
   "source": [
    "### Hessian function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e7ef25b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& g(x, y)=-6 x^4+40 x^3-96 x^2+96 x+2 y-38 \\\\\n",
    "& \\text { General Formula }=\\left|\\begin{array}{ll}\n",
    "f x x & f x y \\\\\n",
    "f x y & f y y\n",
    "\\end{array}\\right| \\\\\n",
    "& =f_{x x} f_{y y}-f^2{ }_{x y} \\\\\n",
    "& f x x=24\\left(-3 x^2+10 x-8\\right) \\\\\n",
    "& f x y=0 \\\\\n",
    "& f x y=0 \\\\\n",
    "& f y y=0 \\\\\n",
    "& H_g=\\left[\\begin{array}{cc}\n",
    "-72 x^2+240 x-192 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193a8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: derive and define the functions\n",
    "def cons_f(x):\n",
    "    return np.array((-2*np.power(x[0], 4) + 8*np.power(x[0], 3) - (8*np.power(x[0], 2)) + x[1] - 2), (-4*np.power(x[0], 4) + 32*np.power(x[0], 3) - 88*np.power(x[0], 2) + 96*x[0] + x[1] - 36))\n",
    "    \n",
    "def cons_Jacobian(x):\n",
    "    # use autograd to compute the Jacobian\n",
    "    return jacobian(cons_f)(x)\n",
    "\n",
    "def cons_Hessian(x, v):\n",
    "    # use autograd to compute the Hessian\n",
    "    return hessian(cons_f)(x)\n",
    "\n",
    "# TODO: Insert the functions above into a NonlinearConstraint obeject\n",
    "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf,0, jac=cons_Jacobian, hess=cons_Hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a3b50",
   "metadata": {},
   "source": [
    "### Define the bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0b7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define the bounds\n",
    "bounds = Bounds([0,0],[3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b1a98",
   "metadata": {},
   "source": [
    "### Call the minimize library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9439bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annabellaputridirgo/opt/anaconda3/envs/CS/lib/python3.10/site-packages/scipy/optimize/_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  warn('delta_grad == 0.0. Check if the approximated '\n"
     ]
    }
   ],
   "source": [
    "x0 = ([0, 1])\n",
    "\n",
    "res = minimize(f, x0, method='trust-constr', constraints=[nonlinear_constraint], bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a8225",
   "metadata": {},
   "source": [
    "### Print out the result you get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6dae800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.12706412 3.9359338 ]\n"
     ]
    }
   ],
   "source": [
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd5cee",
   "metadata": {},
   "source": [
    "### Apply COBYLA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b7e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.12711941 3.93588491]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annabellaputridirgo/opt/anaconda3/envs/CS/lib/python3.10/site-packages/scipy/optimize/_constraints.py:446: OptimizeWarning: Constraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n",
      "  warn(\"Constraint options `finite_diff_jac_sparsity`, \"\n"
     ]
    }
   ],
   "source": [
    "# apply ny coblya method\n",
    "res = minimize(f, x0, method='COBYLA', constraints=[nonlinear_constraint])\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07591de",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cf424f7",
   "metadata": {},
   "source": [
    "### Introduction of the algorithm (Trust Constr Method)\n",
    "The trust-region approach is a solution strategy for unconstrained optimization problems. Trust-region optimization solves unconstrained optimization issues. A trust zone approximates the aim and constraint functions. Trust area points are nearby. Trust regions approximate the aim and constraint functions in the trust region approach. It employs the Byrd-Omojokun Trust-Region SQP technique, which is limited to equality. The trust-region interior point approach is compelled by inequality constraints. By adding slack variables and addressing equality-constrained barrier difficulties with lowering barrier parameters, this interior point technique eliminates inequality restrictions. As the loop nears the answer, the equality restricted SQP technique solves subproblems with increasing precision.\n",
    "\n",
    "### Introduction of the algorithm (COBLYA)\n",
    "Method The Constrained Optimization BY Linear Approximation (COBYLA) approach is used. The approach employs linear approximations to the goal function and each constraint. \n",
    "\n",
    "### Algorithm\n",
    "1. Initialize the trust region radius($r_0$) and barrier parameter, which includes the initial point($x_0$).\n",
    "2. calculate the objective function $f(x)$ and constraint function $cons(x)$ at the initial point $x_0$ and followed with calculating the gradient of the objective function and constraint function at initial point.\n",
    "3. calculate the Lagrangian function $L(x,\\lambda)$ at the initial point $x_0$ and followed with calculating the gradient of the Lagrangian function at initial point.\n",
    "4. calculate the Hessian matrix of the Lagrangian function at the initial point $x_0$.\n",
    "5. calculate the search direction $p$ at the initial point $x_0$.\n",
    "6. calculate the step length $t$ at the initial point $x_0$.\n",
    "7. calculate the new point $x_1$ at the initial point $x_0$.\n",
    "8. calculate the objective function $f(x)$ and constraint function $cons(x)$ at the new point $x_1$ and followed with calculating the gradient of the objective function and constraint function at new point.\n",
    "9. calculate the Lagrangian function $L(x,\\lambda)$ at the new point $x_1$ and followed with calculating the gradient of the Lagrangian function at new point.\n",
    "10. calculate the Hessian matrix of the Lagrangian function at the new point $x_1$.\n",
    "11. calculate the search direction $p$ at the new point $x_1$.\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "H(x_0) & J(x_0)^T \\\\\n",
    "J(x_0) & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "p \\\\\n",
    "\\lambda\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "-\\nabla f(x_0) \\\\\n",
    "-\\nabla cons(x_0)\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "12. calculate the step length $t$ at the new point $x_1$.\n",
    "13. calculate new point by repeating step 7 to 12 until the point is converged.\n",
    "\n",
    "\n",
    "### Comparison\n",
    "The comparison of the two algorithms in this benchmark is trust constr method is more accurate than COBYLA method. The reason is that COBYLA method is a linear approximation method, which is not suitable for this problem. The trust constr method is a quadratic approximation method, which is more suitable for this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c57ec6de47436d79207ffdf5b9e447b71486d02821709d6effdeee695fb5ef9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
